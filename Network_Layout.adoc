= Network layer design for EEA network
:author: Jesus Ruiz
:email: hesus.ruiz@gmail.com
:revnumber: 0.1
:revdate: 01-09-2019
:numbered:
:imagesdir: assets
:icons: font
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:

(Version: {revnumber}, Version Date: {revdate})

== Introduction

Before explaining the proposed network structure, lets refresh how networking works in an Ethereum network, common to all Ethereum clients. We only describe aspects which are relevant to the discussion.

=== Lower level protocols

==== Point-to-point encrypted transport protocol: RLPx

This is a a *TCP-based point-to-point* transport protocol used for communication among Ethereum nodes.
The protocol is *encrypted* and the messages on the wire are created from data structures using the RLP serialization format.

RLP stands for Recursive Lengh Prefix, and its objective is to encode arbitrarily nested arrays of binary data. In this sense, is similar in objectives to Protobuf from Google.

The protocol supports building on top different "capabilities", which enables multiplexing different higher level protocols on top of a single RLPx connection among two nodes.

The most important capability in Ethereum is the "p2p" capability, which is present on all connections. 

==== Node Discovery Protocol

The Node Discovery Protocol is a *UDP-based* protocol. In order to be able to connect to the network, a node has to find a suitable set of peers called neighbours and which is a subset of the whole network.

The neighbours of a node are the other nodes closer to the given node. The concep of proximity is not a geographical one, and a node can have neigbours in any region of the planet (if the network is geographycally distributed).

The Ethereum node discovery protocol is a Kademlia-like Distributed Hash Table (DHT). Every node has an identity, which is its public key, acting as the 'node ID'.

The 'distance' between two node IDs is the bitwise exclusive or on the hashes of the public keys, taken as the number.

    distance(n1, n2) = keccak256(n1) XOR keccak256(n2)

When a new node joins the network, it starts a discovery protocol in order to find a given set of closest neigbours (eg. 25 or 40 nodes), and tries to connect and keep those conections alive.

In order to start the discovery protocol, the node has to be supplied with an initial list of one or more starting nodes called *boot nodes*. Each node in the network maintains a table with its neighbours, and the discovery protocol runs recursively asking the known nodes for its neighbours until it finds the given number of closest ones to the new one, according to the definition of distance given above.

=== Ethereum Wire Protocol (ETH), the p2p protocol

ETH is a protocol which implemente the p2p transmission capabilities of Ethereum, to disseminate transactions and blocks across the whole network, using an "epidemic" transmission protocol.

It uses the RLPx point-to-point transport and the tables of neighbours (peers) built by the Node Discovery Protocol in each of the nodes in the network.

A node keeps connections to a limited number of peers (can be set as a parameter).

When a node receives a transaction from another node, it resends the transaction to its peers, which do the same with its own peers (following some rules to avoid overflowing the network with redundant messages).

Block propagation follows the same basic mechanism, with some optimizations given the bigger size of messages compared to re-sending transactions. Basically, the full blocks are sent to a small fraction of connected peers (the square root of the total number of peers), while the other connected peers receive only the hashes of the blocks and they have to ask for the full block when they need it.

=== Permissioning and Static nodes

In a permissioned network, the EThereum clients provide some capabilities not normally used in permissionless networks enabling some flexibility on "shaping" the network.

==== Permissioning

In permissioned networks, the Ethereum clients provide the ability to define for each node a list of "permissioned nodes".

The list of permissioned nodes specify the node IDs that will be accepted by a given node if they try to start a connection with that node.

Currently, both Quorum and Pantheon support a per-node permission list, which means that each node in the network can have a different list of nodes which are permitted to connect.

==== Static nodes list

This is a per-node list. The list of static nodes specifies the nodes with which the given node will always try to connect and keep the connection alive. The nodes in this list may be any node in the network and not necessarily neighbours. The nodes in this list are connected directly at the TCP level and so sending messages (transactions or blocks) do not need re-transmissions with the p2p protocol.

== Some communication-related problems to solve in big permissioned Ethereum networks

In this document I will use the term "consensus node" to refer to the nodes that participate and execute the consensus algorithm. This is to make the term more general than Validators an even applicable to other blockchain networks like Fabric (they are called Orderers). In addition, the term Validator is misleading, because IBFT consensus nodes have the mission to order transactions and create blocks, and validation is ultimately performed in each and every client, because the the clients do not trust at all on consensus nodes.

In a big permissioned Ethereum network the consensus nodes are subject to a lot of overhead because they have to do several things at the same time:

* Executing the consensus algorithm itsef. This is really the core mission of the consensus nodes.

* In order to do so, they have to stablish and maintain the connections with all the other consensus nodes, in order to execute the consensus algorithm. Ideally, this should be managed with point-to-point connections, and not using the p2p transmission.

* Stablishing and tear-down of TCP communications with all the peers that try to stablish connections with them for the P2P dissemination of transactions and blocks. If the peers of consensus nodes are appearing and dissapearing, the pattern of communications is not very stable and causes a lot of work at the network stack level.

In addition, maintenance of the nodes allowed to participate in the network is difficult, given the nature of the per-node lists mentioned above.

== Proposed network structure

To aid in the explanation, let me use as an example the structure of an actual public-permissioned network with some 100 nodes in production (Alastria RedT, which uses Quorum but the concepts are equally translated a Pantheon network).

image::redt_visual.png[]

In this network all nodes are running exactly the same client software (in this case Quorum), but using the permissioning and static nodes capabilities, three layers are defined with nodes specialized in different functions.

=== Regular nodes

The Regular nodes are the nodes operated by all the members of the network. They are used for injecting transactions and reading from the blockchain. The Regular nodes are normally full nodes that keep a copy of the blockchain, to reduce the level of trust required from each member with respect to the rest of the network. However, the members are free to maintain as much history of the blockchain as they wish, to minimize storage in the nodes. A separate document describes how such mechanisms could be implemented in a public-permissioned network in such a way that the mechanism is effective while minimizing the trust requirements and maximizing decentralization.

=== Consensus nodes

The Consensus nodes are specialized on executing the consensus algorithm. The static-nodes list and the permissioned-nodes lists are used to specify permanent point-to-point connections among the consensus nodes, in order to ensure a very stable communications pattern. The Consensus nodes are also connected with the Boot/Permissioning nodes. The permissioned-nodes list is used to ensure that the Regular nodes can not connect dsirectly with the Consensus nodes.

=== Boot/Permissioning nodes

The Boot/Permissioning nodes (from now on called just Permissioning nodes) are used for three related purposes:

. *Bootstrapping*, that is, for helping new nodes to connect to the network. The boot nodes are in a stable and well-known list of nodes that anybody can use to connect to the network. For example, in the public Ethereum network there are 3 boot nodes maintained by the Ethereum foundation exactly for this purpose. In the network depicted above we can see 5 boot nodes, and by default the Regular nodes have these nodes specified in the static-nodes list. But this is not compulsory, and some members could collaborate among each other and define themselves static connections among their nodes if they feel the need.

. *Permissioning at the network level*. The boot nodes have all the same permissioning list with all Regular nodes in the network. That is, when a new Regular node is added to the network, it is added to this list.

. *Isolating Consensus nodes*. Regular nodes are allowed to connect directly at the TCP level to Permissioning nodes, but not to Consensus nodes. Please remember that thanks to the p2p protocol, messages can reach all nodes of the network without direct connectivity among them. This provides to the Consensus nodes a very stable environment for the efficient execution of the consensus algorithm.

== Additional measures to increment network resiliency and inclusiveness

The diagram above depicts 7 consensus nodes, but this is not fully correct. The network is implementing a novel on-chain governance of Consensus nodes execution, which we call "IBFT with proactive rotation and recovery", which is an enhancement of some proposed extensions in the literature, most notably the famous "Practical Byzantine Fault Tolerance and Proactive Recovery" (Castro and Liskov 2002).

Standard implementations of PBFT and in particular IBFT in Quorum and Pantheon, tend to focus on masking failures. That is, they hide failures making them transparent to the users because the network continues working, but they do not manage those failures in a way that proactive or reactive measures can be taken to ensure the long-term health of the network.

This is the reason why Alastria is implementing a set of tools surrounding the base IBFT consensus algorithm, which together with complementary off-chain governance processes allow the realization of
the the principles of the governance of the blockchain as a Common-Pool Resource.

The mechanism is illustrated in the following diagram.

image::OnChainConsensus.jpg[]

The following aspects can be observed:

* The consensus nodes in the Active state (that is, actually executing the base
IBFT algorithm) are being monitored.

* The events signaling different types of faults are used for the reactive governance of the nodes. Even though it is not shown in the figure, in addition to the automated reaction, the events should be reported in a way that any participant in the blockchain network (not just the consensus nodes) can know what is happening at any moment. This is required to implement the high levels of transparency and collaborative monitoring that are required in such a network.

* Depending on the severity of the fault detected (crash or byzantine), the system reacts automatically applying a graduated set of sanctions. For example, when the fault is byzantine (which means a malicious behaviour is suspected), the consensus node affected is put in quarantine, effectively stopping the node from participating in the consensus execution. If the owner is willing to continue participating, a manual process (off-chain governance) is required, with sufficient explanation and justification to the other members in order to be accepted again. An example of a byzantine behaviour that can be detected is a node sending two different blocks with the same block number to different nodes. The blocks ara digitally signed so the malicious action can not be denied if detected.

== Discussion about the proposed structure

=== Effect on network throughput

The proposed structure should not have an adverse effect on network throughput. On the contrary, it should enable to increase it and keep it very stable.

In an Ethereum network, there are typically two bottlenecks for network throughput.

The first one it the *performance of the consensus algorithm*, and this is what is observed in the public Ethereum network. In a permissioned network with IBFT, the speed of execution of the consensus algorithm is heavily affected by the communication message complexity among consensus nodes (increases with the square of the number of consensus nodes), in other words it is *network-bound*. The proposed network structure provides to the consensus nodes a very stable environment for the execution of the consensus algorithm, free from the overheads of random connections and disconnections from Regular nodes, which can come and go as the members wish. There is of course no compromise or similar SLA for operating the Regular nodes, and the members are free to start and stop them at will. This means that the proposed structure is ideal for eliminating the first bottleneck.

The second bottleneck is *CPU-bound* and caused because all nodes in the network have to execute all the transactions injected by all nodes in the network. This is not a problem in the public Ethereum network, because of the first bottleneck. But in a permissioned network with very fast consensus algorithm execution (big consensus machines and good connectivity among consensus nodes), it is very likely that Regular nodes hit the second bottleneck, especially if the Regular nodes have significantly less powerful machines than the Consensus nodes.

In a permissioned network with some geographic locality and relatively good communication links among all participating nodes with acceptable latencies (eg Europe), it is highly unlikely that adding the requirement that Regular nodes need two communication hops to contact a Consensu node is going to affect network performance, because the two bottlenecks mentioned before are much more prominent.

=== Resiliency of the network

It could be argued that the resiliency of the network is reduced, because a p2p network has been converted to a hierarchical network with a small number of nodes (the boot nodes) being a single point of failure.

Actually, the real resiliency (and safety and liveness) of the network depends fundamentally on the set of consensus nodes operating continuosly and in a manner that they are safe from the attacks from malicious actors. The proposed structure, even though currently does not cover every attck, is well suited to implement additional measures of defense against attacks. For example, the consensus nodes could have two network adapters and the consensus algorithm could be running on a specialized network protected from the rest of the world by specialized firewalls (VPN), making the whole network much more resilient.

In the proposed structure, the boot nodes are just "proxies" or "pass-through" to isolate the Consensus nodes from direct connections from Regular nodes. They are all interconnected, so as long as there is one alive, the network continues working. It is very easy to launch additional nodes if required, so it is advisable to monitor proactively the health of boot nodes and manage proactively the number of nodes active at any given moment. In case of extreme need, it is enough to update the permissioning list in Consensus nodes to allow Regular nodes to connect directly, something that can be done without stopping the nodes. But given that it is easier to launch new Boot nodes, this is probably never needed.

In principle, this could be achieved with specialized software, and this may be a line of future research. However, for simplicity of implementation, and taking into account that the Boot nodes ara also Permissioning nodes, we chose to use exactly the same client software to implement that function.

